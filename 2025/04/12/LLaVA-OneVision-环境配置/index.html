
<!DOCTYPE html>
<html lang="Zh-CN">
<head>
    <meta charset="utf-8" />
    <title>LLaVA-OneVision 环境配置 | SharlotAway</title>
    <meta name="author" content="Sharlot Away" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/avatar_f.jpg" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>



<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading_furina.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>SHARLOTAWAY</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about/">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags/">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;SHARLOTAWAY</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>LLaVA-OneVision 环境配置</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/4/12
        </span>
        
        
        <span class="tags">
            <span class="icon">
                <i class="fa-solid fa-tags fa-fw"></i>
            </span>
            
            
            <span class="tag">
                
                <a href="/tags/Environment/" style="color: #03a9f4">
                    Environment
                </a>
            </span>
            
        </span>
        
    </div>
    
    <div class="content" v-pre>
        <h2 id="加载模型出现-no-op，推理会终止进程"><a href="#加载模型出现-no-op，推理会终止进程" class="headerlink" title="加载模型出现 no-op，推理会终止进程"></a>加载模型出现 no-op，推理会终止进程</h2><p>在修改了 <code>siglip_encoder</code> 的逻辑后无法正常推理，使用 <code>pip</code> 重装 <code>llava</code> 模块的之后加载模型就出现了问题。具体是所有的 vision_model 的参数都显示正在将 <code>non-meta</code> 参数加载到 <code>meta</code> 参数。</p>
<pre><code class="python">/home/24_zhangxiao/environment/anaconda3/envs/llavaOV/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f&#39;for &#123;key&#125;: copying from a non-meta parameter in the checkpoint to a meta &#39;
</code></pre>
<p>暂未找到合理的解决方案。</p>
<h2 id="重新配置-llava-onevision-代码"><a href="#重新配置-llava-onevision-代码" class="headerlink" title="重新配置 llava-onevision 代码"></a>重新配置 llava-onevision 代码</h2><h3 id="1-下载代码，配置环境"><a href="#1-下载代码，配置环境" class="headerlink" title="1.  下载代码，配置环境"></a>1.  下载代码，配置环境</h3><p>下载 github 仓库。</p>
<pre><code class="bash">git clone https://github.com/LLaVA-VL/LLaVA-NeXT
cd LLaVA-NeXT
</code></pre>
<p>配置 conda 环境。</p>
<pre><code class="bash">conda create -n llava python=3.10 -y
conda activate llava
pip install --upgrade pip  # Enable PEP 660 support.
pip install -e &quot;.[train]&quot;
</code></pre>
<p>安装  flash attention，考虑到服务器 cuda 版本为 cu117，因此需要降级。</p>
<pre><code class="bash">pip install flash_attn==2.7.3 --no-build-isolation
</code></pre>
<h3 id="2-测试代码"><a href="#2-测试代码" class="headerlink" title="2. 测试代码"></a>2. 测试代码</h3><h3 id="2-0-依赖版本调试"><a href="#2-0-依赖版本调试" class="headerlink" title="2.0 依赖版本调试"></a>2.0 依赖版本调试</h3><p>从 <code>model_vqa</code> 开始，运行代码产生报错如下，怀疑 cuda\torch\flash_attn 版本不对应。</p>
<pre><code class="python">Failed to import llava_qwen from llava.language_model.llava_qwen. Error: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/home/24_zhangxiao/environment/anaconda3/envs/llava/lib/python3.10/site-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZNK3c105Error4whatEv
</code></pre>
<p>重新安装 torch，从 pytorch 官网的<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/previous-versions/">previous versions</a> 的网页找到对应版本的下载指令。</p>
<pre><code class="bash">pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2
</code></pre>
<p>从 flash attention 的 <a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/releases">github 仓库</a>查找适合 torch2.0 的版本，跳转指定目录后 <code>pip</code> 安装。</p>
<pre><code class="bash">pip install flash_attn-2.6.3+cu118torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl 
</code></pre>
<p>2.1 checkpoint 参数调试</p>
<p>经过上面的配置，可以成功开始载入模型。</p>
<blockquote>
<p>  为了确保正确检索，可以在代码中添加 sys 指定到项目地址</p>
<pre><code class="python"># 需要定义llava模块的位置，即指定PATH变量
import sys
llava_module_dir = &quot;/path/to/LLaVA-NeXT&quot;
sys.path.insert(0, llava_module_dir)
</code></pre>
</blockquote>
<p>由于笔者使用 <code>lmms-lab/llava-onevision-qwen2-0.5b-ov</code> （<a target="_blank" rel="noopener" href="https://huggingface.co/lmms-lab/llava-onevision-qwen2-0.5b-ov">模型地址</a>），使用的视觉编码器是<code>google/siglip-so400m-patch14-384</code>（<a target="_blank" rel="noopener" href="https://huggingface.co/google/siglip-so400m-patch14-384">模型地址</a>），直接使用原始代码加载模型出现报错<code>size mismatch</code>，完整报错代码如下。</p>
<pre><code class="python">RuntimeError: Error(s) in loading state_dict for CLIPVisionModel:
        size mismatch for vision_model.embeddings.patch_embedding.weight: copying a param with shape torch.Size([1152, 3, 14, 14]) from checkpoint, the shape in current model is torch.Size([768, 3, 32, 32]).
</code></pre>
<p>由于加载视觉编码器逻辑设置问题，错误加载成了 <code>CLIPVisionModel</code>，模型参数无法对齐。</p>
<pre><code class="python">  File &quot;/home/24_zhangxiao/environment/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py&quot;, line 3404, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File &quot;/home1/zhangxiao/LLaVA-OneVision/LLaVA-NeXT/llava/model/language_model/llava_qwen.py&quot;, line 55, in __init__
    self.model = LlavaQwenModel(config)
  File &quot;/home1/zhangxiao/LLaVA-OneVision/LLaVA-NeXT/llava/model/language_model/llava_qwen.py&quot;, line 43, in __init__
    super(LlavaQwenModel, self).__init__(config)
  File &quot;/home1/zhangxiao/LLaVA-OneVision/LLaVA-NeXT/llava/model/llava_arch.py&quot;, line 41, in __init__
    self.vision_tower = build_vision_tower(config, delay_load=delay_load)
  File &quot;/home1/zhangxiao/LLaVA-OneVision/LLaVA-NeXT/llava/model/multimodal_encoder/builder.py&quot;, line 21, in build_vision_tower
    return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)
  File &quot;/home1/zhangxiao/LLaVA-OneVision/LLaVA-NeXT/llava/model/multimodal_encoder/clip_encoder.py&quot;, line 24, in __init__
    self.load_model()
  File &quot;/home1/zhangxiao/LLaVA-OneVision/LLaVA-NeXT/llava/model/multimodal_encoder/clip_encoder.py&quot;, line 41, in load_model
    self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)
</code></pre>
<p>修改 <code>/home1/zhangxiao/LLaVA-OneVision/LLaVA-NeXT/llava/model/multimodal_encoder/builder.py</code> 的代码逻辑（暂且简单将 siglip 置顶）</p>
<pre><code class="python">from .siglip_encoder import SigLipVisionTower

def build_vision_tower(vision_tower_cfg, **kwargs):
    vision_tower = getattr(vision_tower_cfg, &quot;mm_vision_tower&quot;, getattr(vision_tower_cfg, &quot;vision_tower&quot;, None))
    is_absolute_path_exists = os.path.exists(vision_tower)
    use_s2 = getattr(vision_tower_cfg, &quot;s2&quot;, False)
    if &quot;siglip&quot; in vision_tower:
        return SigLipVisionTower(vision_tower, vision_tower_cfg=vision_tower_cfg, **kwargs)
</code></pre>
<p>在 <code>llava.model.builder</code> 中添加与 Qwen 相关的代码，主要是加载配置，用于合并 <code>lora</code> 权重等功能。</p>
<pre><code class="python">elif &quot;qwen&quot; in model_name.lower() or &quot;quyen&quot; in model_name.lower():
    tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)
    if &quot;moe&quot;  in model_name.lower() or &quot;A14B&quot; in model_name.lower():
        from llava.model.language_model.llava_qwen_moe import LlavaQwenMoeConfig
        if overwrite_config is not None:
            llava_cfg = LlavaQwenMoeConfig.from_pretrained(model_path)
            rank0_print(f&quot;Overwriting config with &#123;overwrite_config&#125;&quot;)
            for k, v in overwrite_config.items():
                setattr(llava_cfg, k, v)
                lora_cfg_pretrained = LlavaQwenMoeConfig.from_pretrained(model_path)
                model = LlavaQwenMoeForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=llava_cfg, attn_implementation=attn_implementation, **kwargs)
        else:
            lora_cfg_pretrained = LlavaQwenMoeConfig.from_pretrained(model_path)
            model = LlavaQwenMoeForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)

    else:
        from llava.model.language_model.llava_qwen import LlavaQwenConfig
        if overwrite_config is not None:
            llava_cfg = LlavaQwenConfig.from_pretrained(model_path)
            rank0_print(f&quot;Overwriting config with &#123;overwrite_config&#125;&quot;)
            for k, v in overwrite_config.items():
                setattr(llava_cfg, k, v)
                lora_cfg_pretrained = LlavaQwenConfig.from_pretrained(model_path)
                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)
                model = LlavaQwenForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)
        else:
            lora_cfg_pretrained = LlavaQwenConfig.from_pretrained(model_path)
            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)
            model = LlavaQwenForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)
</code></pre>
<p>接下来就是修改一下数据格式即可正常加载。</p>
<p><img src="/2025/04/12/LLaVA-OneVision-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/image-20250407165615030.png" alt="image-20250407165615030"></p>
<h3 id="2-2-训练代码调试"><a href="#2-2-训练代码调试" class="headerlink" title="2.2. 训练代码调试"></a>2.2. 训练代码调试</h3><p>调好测试代码之后，尝试运行训练代码。出现的第一个报错是 <code>accelerate</code> 的错误。</p>
<pre><code class="python">TypeError: Accelerator.__init__() got an unexpected keyword argument &#39;dispatch_batches&#39;
</code></pre>
<p>疑似 <code>accelerate</code> 不兼容，重装 <code>accelerate</code></p>
<pre><code class="python">Installing collected packages: accelerate
  Attempting uninstall: accelerate
    Found existing installation: accelerate 1.6.0
    Uninstalling accelerate-1.6.0:
      Successfully uninstalled accelerate-1.6.0
Successfully installed accelerate-0.29.3
</code></pre>
<p>成功加载，但是 OOM ，静候显卡空出。</p>
<h3 id="2-3-评价指标运行"><a href="#2-3-评价指标运行" class="headerlink" title="2.3. 评价指标运行"></a>2.3. 评价指标运行</h3><p>需要安装的包：<code>nltk</code>, <code>rouge</code>, <code>pycocoevalcap</code>,<code>jieba</code></p>

    </div>
    
    
    
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2025 - 2025 SharlotAway
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Sharlot Away
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
    




    
</body>
</html>
